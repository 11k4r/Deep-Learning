{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fid_selection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "zTx7uJMMKKHY"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7SjjMvaJ66Z"
      },
      "source": [
        "#if use google drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olXCSwnfKAkZ"
      },
      "source": [
        "root = '/content/drive/MyDrive/gan-getting-started/monet_jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8BS1ISiaJIe9"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "try:\n",
        "    from torchvision.models.utils import load_state_dict_from_url\n",
        "except ImportError:\n",
        "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
        "\n",
        "# Inception weights ported to Pytorch from\n",
        "# http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz\n",
        "FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'  # noqa: E501\n",
        "\n",
        "\n",
        "class InceptionV3(nn.Module):\n",
        "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
        "\n",
        "    # Index of default block of inception to return,\n",
        "    # corresponds to output of final average pooling\n",
        "    DEFAULT_BLOCK_INDEX = 3\n",
        "\n",
        "    # Maps feature dimensionality to their output blocks indices\n",
        "    BLOCK_INDEX_BY_DIM = {\n",
        "        64: 0,   # First max pooling features\n",
        "        192: 1,  # Second max pooling featurs\n",
        "        768: 2,  # Pre-aux classifier features\n",
        "        2048: 3  # Final average pooling features\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "                 output_blocks=(DEFAULT_BLOCK_INDEX,),\n",
        "                 resize_input=True,\n",
        "                 normalize_input=True,\n",
        "                 requires_grad=False,\n",
        "                 use_fid_inception=True):\n",
        "        \"\"\"Build pretrained InceptionV3\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_blocks : list of int\n",
        "            Indices of blocks to return features of. Possible values are:\n",
        "                - 0: corresponds to output of first max pooling\n",
        "                - 1: corresponds to output of second max pooling\n",
        "                - 2: corresponds to output which is fed to aux classifier\n",
        "                - 3: corresponds to output of final average pooling\n",
        "        resize_input : bool\n",
        "            If true, bilinearly resizes input to width and height 299 before\n",
        "            feeding input to model. As the network without fully connected\n",
        "            layers is fully convolutional, it should be able to handle inputs\n",
        "            of arbitrary size, so resizing might not be strictly needed\n",
        "        normalize_input : bool\n",
        "            If true, scales the input from range (0, 1) to the range the\n",
        "            pretrained Inception network expects, namely (-1, 1)\n",
        "        requires_grad : bool\n",
        "            If true, parameters of the model require gradients. Possibly useful\n",
        "            for finetuning the network\n",
        "        use_fid_inception : bool\n",
        "            If true, uses the pretrained Inception model used in Tensorflow's\n",
        "            FID implementation. If false, uses the pretrained Inception model\n",
        "            available in torchvision. The FID Inception model has different\n",
        "            weights and a slightly different structure from torchvision's\n",
        "            Inception model. If you want to compute FID scores, you are\n",
        "            strongly advised to set this parameter to true to get comparable\n",
        "            results.\n",
        "        \"\"\"\n",
        "        super(InceptionV3, self).__init__()\n",
        "\n",
        "        self.resize_input = resize_input\n",
        "        self.normalize_input = normalize_input\n",
        "        self.output_blocks = sorted(output_blocks)\n",
        "        self.last_needed_block = max(output_blocks)\n",
        "\n",
        "        assert self.last_needed_block <= 3, \\\n",
        "            'Last possible output block index is 3'\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        if use_fid_inception:\n",
        "            inception = fid_inception_v3()\n",
        "        else:\n",
        "            inception = _inception_v3(pretrained=True)\n",
        "\n",
        "        # Block 0: input to maxpool1\n",
        "        block0 = [\n",
        "            inception.Conv2d_1a_3x3,\n",
        "            inception.Conv2d_2a_3x3,\n",
        "            inception.Conv2d_2b_3x3,\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "        ]\n",
        "        self.blocks.append(nn.Sequential(*block0))\n",
        "\n",
        "        # Block 1: maxpool1 to maxpool2\n",
        "        if self.last_needed_block >= 1:\n",
        "            block1 = [\n",
        "                inception.Conv2d_3b_1x1,\n",
        "                inception.Conv2d_4a_3x3,\n",
        "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block1))\n",
        "\n",
        "        # Block 2: maxpool2 to aux classifier\n",
        "        if self.last_needed_block >= 2:\n",
        "            block2 = [\n",
        "                inception.Mixed_5b,\n",
        "                inception.Mixed_5c,\n",
        "                inception.Mixed_5d,\n",
        "                inception.Mixed_6a,\n",
        "                inception.Mixed_6b,\n",
        "                inception.Mixed_6c,\n",
        "                inception.Mixed_6d,\n",
        "                inception.Mixed_6e,\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block2))\n",
        "\n",
        "        # Block 3: aux classifier to final avgpool\n",
        "        if self.last_needed_block >= 3:\n",
        "            block3 = [\n",
        "                inception.Mixed_7a,\n",
        "                inception.Mixed_7b,\n",
        "                inception.Mixed_7c,\n",
        "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
        "            ]\n",
        "            self.blocks.append(nn.Sequential(*block3))\n",
        "\n",
        "        for param in self.parameters():\n",
        "            param.requires_grad = requires_grad\n",
        "\n",
        "    def forward(self, inp):\n",
        "        \"\"\"Get Inception feature maps\n",
        "        Parameters\n",
        "        ----------\n",
        "        inp : torch.autograd.Variable\n",
        "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
        "            range (0, 1)\n",
        "        Returns\n",
        "        -------\n",
        "        List of torch.autograd.Variable, corresponding to the selected output\n",
        "        block, sorted ascending by index\n",
        "        \"\"\"\n",
        "        outp = []\n",
        "        x = inp\n",
        "\n",
        "        if self.resize_input:\n",
        "            x = F.interpolate(x,\n",
        "                              size=(299, 299),\n",
        "                              mode='bilinear',\n",
        "                              align_corners=False)\n",
        "\n",
        "        if self.normalize_input:\n",
        "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
        "\n",
        "        for idx, block in enumerate(self.blocks):\n",
        "            x = block(x)\n",
        "            if idx in self.output_blocks:\n",
        "                outp.append(x)\n",
        "\n",
        "            if idx == self.last_needed_block:\n",
        "                break\n",
        "\n",
        "        return outp\n",
        "\n",
        "\n",
        "def _inception_v3(*args, **kwargs):\n",
        "    \"\"\"Wraps `torchvision.models.inception_v3`\n",
        "    Skips default weight inititialization if supported by torchvision version.\n",
        "    See https://github.com/mseitzer/pytorch-fid/issues/28.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        version = tuple(map(int, torchvision.__version__.split('.')[:2]))\n",
        "    except ValueError:\n",
        "        # Just a caution against weird version strings\n",
        "        version = (0,)\n",
        "\n",
        "    if version >= (0, 6):\n",
        "        kwargs['init_weights'] = False\n",
        "\n",
        "    return torchvision.models.inception_v3(*args, **kwargs)\n",
        "\n",
        "\n",
        "def fid_inception_v3():\n",
        "    \"\"\"Build pretrained Inception model for FID computation\n",
        "    The Inception model for FID computation uses a different set of weights\n",
        "    and has a slightly different structure than torchvision's Inception.\n",
        "    This method first constructs torchvision's Inception and then patches the\n",
        "    necessary parts that are different in the FID Inception model.\n",
        "    \"\"\"\n",
        "    inception = _inception_v3(num_classes=1008,\n",
        "                              aux_logits=False,\n",
        "                              pretrained=False)\n",
        "    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)\n",
        "    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)\n",
        "    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)\n",
        "    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)\n",
        "    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)\n",
        "    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)\n",
        "    inception.Mixed_7b = FIDInceptionE_1(1280)\n",
        "    inception.Mixed_7c = FIDInceptionE_2(2048)\n",
        "\n",
        "    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)\n",
        "    inception.load_state_dict(state_dict)\n",
        "    return inception\n",
        "\n",
        "\n",
        "class FIDInceptionA(torchvision.models.inception.InceptionA):\n",
        "    \"\"\"InceptionA block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, pool_features):\n",
        "        super(FIDInceptionA, self).__init__(in_channels, pool_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch5x5 = self.branch5x5_1(x)\n",
        "        branch5x5 = self.branch5x5_2(branch5x5)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionC(torchvision.models.inception.InceptionC):\n",
        "    \"\"\"InceptionC block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels, channels_7x7):\n",
        "        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch7x7 = self.branch7x7_1(x)\n",
        "        branch7x7 = self.branch7x7_2(branch7x7)\n",
        "        branch7x7 = self.branch7x7_3(branch7x7)\n",
        "\n",
        "        branch7x7dbl = self.branch7x7dbl_1(x)\n",
        "        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)\n",
        "        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_1(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"First InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_1, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: Tensorflow's average pool does not use the padded zero's in\n",
        "        # its average calculation\n",
        "        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1,\n",
        "                                   count_include_pad=False)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)\n",
        "\n",
        "\n",
        "class FIDInceptionE_2(torchvision.models.inception.InceptionE):\n",
        "    \"\"\"Second InceptionE block patched for FID computation\"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(FIDInceptionE_2, self).__init__(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        branch1x1 = self.branch1x1(x)\n",
        "\n",
        "        branch3x3 = self.branch3x3_1(x)\n",
        "        branch3x3 = [\n",
        "            self.branch3x3_2a(branch3x3),\n",
        "            self.branch3x3_2b(branch3x3),\n",
        "        ]\n",
        "        branch3x3 = torch.cat(branch3x3, 1)\n",
        "\n",
        "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
        "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
        "        branch3x3dbl = [\n",
        "            self.branch3x3dbl_3a(branch3x3dbl),\n",
        "            self.branch3x3dbl_3b(branch3x3dbl),\n",
        "        ]\n",
        "        branch3x3dbl = torch.cat(branch3x3dbl, 1)\n",
        "\n",
        "        # Patch: The FID Inception model uses max pooling instead of average\n",
        "        # pooling. This is likely an error in this specific Inception\n",
        "        # implementation, as other Inception models use average pooling here\n",
        "        # (which matches the description in the paper).\n",
        "        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)\n",
        "        branch_pool = self.branch_pool(branch_pool)\n",
        "\n",
        "        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]\n",
        "        return torch.cat(outputs, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvMA7gI9JK2M"
      },
      "source": [
        "import os\n",
        "import pathlib\n",
        "from multiprocessing import cpu_count\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as TF\n",
        "from PIL import Image\n",
        "from scipy import linalg\n",
        "from torch.nn.functional import adaptive_avg_pool2d\n",
        "\n",
        "#from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "viN9eioWJUbd"
      },
      "source": [
        "class ImagePathDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, files, transforms=None):\n",
        "        self.files = files\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        path = self.files[i]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        if self.transforms is not None:\n",
        "            img = self.transforms(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "def get_activations(files, model, batch_size=50, dims=2048, device='cpu', num_workers=8):\n",
        "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
        "    Params:\n",
        "    -- files       : List of image files paths\n",
        "    -- model       : Instance of inception model\n",
        "    -- batch_size  : Batch size of images for the model to process at once.\n",
        "                     Make sure that the number of samples is a multiple of\n",
        "                     the batch size, otherwise some samples are ignored. This\n",
        "                     behavior is retained to match the original FID score\n",
        "                     implementation.\n",
        "    -- dims        : Dimensionality of features returned by Inception\n",
        "    -- device      : Device to run calculations\n",
        "    -- num_workers : Number of parallel dataloader workers\n",
        "    Returns:\n",
        "    -- A numpy array of dimension (num images, dims) that contains the\n",
        "       activations of the given tensor when feeding inception with the\n",
        "       query tensor.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    if batch_size > len(files):\n",
        "        print(('Warning: batch size is bigger than the data size. '\n",
        "               'Setting batch size to data size'))\n",
        "        batch_size = len(files)\n",
        "\n",
        "    dataset = ImagePathDataset(files, transforms=TF.ToTensor())\n",
        "    dataloader = torch.utils.data.DataLoader(dataset,\n",
        "                                             batch_size=batch_size,\n",
        "                                             shuffle=False,\n",
        "                                             drop_last=False,\n",
        "                                             num_workers=num_workers)\n",
        "\n",
        "    pred_arr = np.empty((len(files), dims))\n",
        "\n",
        "    start_idx = 0\n",
        "\n",
        "    for batch in tqdm(dataloader):\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(batch)[0]\n",
        "\n",
        "        # If model output is not scalar, apply global spatial average pooling.\n",
        "        # This happens if you choose a dimensionality not equal 2048.\n",
        "        if pred.size(2) != 1 or pred.size(3) != 1:\n",
        "            pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
        "\n",
        "        pred = pred.squeeze(3).squeeze(2).cpu().numpy()\n",
        "\n",
        "        pred_arr[start_idx:start_idx + pred.shape[0]] = pred\n",
        "\n",
        "        start_idx = start_idx + pred.shape[0]\n",
        "\n",
        "    return pred_arr\n",
        "\n",
        "\n",
        "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
        "    \"\"\"Numpy implementation of the Frechet Distance.\n",
        "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
        "    and X_2 ~ N(mu_2, C_2) is\n",
        "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
        "    Stable version by Dougal J. Sutherland.\n",
        "    Params:\n",
        "    -- mu1   : Numpy array containing the activations of a layer of the\n",
        "               inception net (like returned by the function 'get_predictions')\n",
        "               for generated samples.\n",
        "    -- mu2   : The sample mean over activations, precalculated on an\n",
        "               representative data set.\n",
        "    -- sigma1: The covariance matrix over activations for generated samples.\n",
        "    -- sigma2: The covariance matrix over activations, precalculated on an\n",
        "               representative data set.\n",
        "    Returns:\n",
        "    --   : The Frechet Distance.\n",
        "    \"\"\"\n",
        "\n",
        "    mu1 = np.atleast_1d(mu1)\n",
        "    mu2 = np.atleast_1d(mu2)\n",
        "\n",
        "    sigma1 = np.atleast_2d(sigma1)\n",
        "    sigma2 = np.atleast_2d(sigma2)\n",
        "\n",
        "    assert mu1.shape == mu2.shape, \\\n",
        "        'Training and test mean vectors have different lengths'\n",
        "    assert sigma1.shape == sigma2.shape, \\\n",
        "        'Training and test covariances have different dimensions'\n",
        "\n",
        "    diff = mu1 - mu2\n",
        "\n",
        "    # Product might be almost singular\n",
        "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
        "    if not np.isfinite(covmean).all():\n",
        "        msg = ('fid calculation produces singular product; '\n",
        "               'adding %s to diagonal of cov estimates') % eps\n",
        "        print(msg)\n",
        "        offset = np.eye(sigma1.shape[0]) * eps\n",
        "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
        "\n",
        "    # Numerical error might give slight imaginary component\n",
        "    if np.iscomplexobj(covmean):\n",
        "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
        "            m = np.max(np.abs(covmean.imag))\n",
        "            raise ValueError('Imaginary component {}'.format(m))\n",
        "        covmean = covmean.real\n",
        "\n",
        "    tr_covmean = np.trace(covmean)\n",
        "\n",
        "    return (diff.dot(diff) + np.trace(sigma1)\n",
        "            + np.trace(sigma2) - 2 * tr_covmean)\n",
        "\n",
        "\n",
        "def calculate_activation_statistics(files, model, batch_size=50, dims=2048,\n",
        "                                    device='cpu', num_workers=8):\n",
        "    \"\"\"Calculation of the statistics used by the FID.\n",
        "    Params:\n",
        "    -- files       : List of image files paths\n",
        "    -- model       : Instance of inception model\n",
        "    -- batch_size  : The images numpy array is split into batches with\n",
        "                     batch size batch_size. A reasonable batch size\n",
        "                     depends on the hardware.\n",
        "    -- dims        : Dimensionality of features returned by Inception\n",
        "    -- device      : Device to run calculations\n",
        "    -- num_workers : Number of parallel dataloader workers\n",
        "    Returns:\n",
        "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
        "               the inception model.\n",
        "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
        "               the inception model.\n",
        "    \"\"\"\n",
        "    act = get_activations(files, model, batch_size, dims, device, num_workers)\n",
        "    mu = np.mean(act, axis=0)\n",
        "    sigma = np.cov(act, rowvar=False)\n",
        "    return mu, sigma\n",
        "\n",
        "\n",
        "def compute_statistics_of_path(imgs, path, model, batch_size, dims, device, num_workers=8):\n",
        "    if path.endswith('.npz'):\n",
        "        with np.load(path) as f:\n",
        "            m, s = f['mu'][:], f['sigma'][:]\n",
        "    else:\n",
        "        path = pathlib.Path(path)\n",
        "        files = sorted([file for file in path.glob('*{}'.format(images)) if (str(file)).split('/')[-1] in imgs])\n",
        "        m, s = calculate_activation_statistics(files, model, batch_size,\n",
        "                                               dims, device, num_workers)\n",
        "\n",
        "    return m, s\n",
        "\n",
        "\n",
        "def calculate_fid_given_paths(imgs, paths, batch_size, device, dims, num_workers=8):\n",
        "    \"\"\"Calculates the FID of two paths\"\"\"\n",
        "    for p in paths:\n",
        "        if not os.path.exists(p):\n",
        "            raise RuntimeError('Invalid path: %s' % p)\n",
        "\n",
        "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
        "\n",
        "    model = InceptionV3([block_idx]).to(device)\n",
        "\n",
        "    # m1, s1 = compute_statistics_of_path(paths[0], model, batch_size,\n",
        "    #                                     dims, device, num_workers)\n",
        "    m2, s2 = compute_statistics_of_path(imgs, paths[1], model, batch_size,\n",
        "                                        dims, device, num_workers)\n",
        "    fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
        "\n",
        "    return fid_value\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjNce3YELdjJ"
      },
      "source": [
        "device = torch.device('cuda' if (torch.cuda.is_available()) else 'cpu')\n",
        "images = np.array(os.listdir(root))\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOVzFIjR97Df"
      },
      "source": [
        "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
        "\n",
        "model = InceptionV3([block_idx]).to(device)\n",
        "\n",
        "m1, s1 = compute_statistics_of_path(images, root, model, 1,\n",
        "                                        2048, device, 8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Eye96PjJkC5"
      },
      "source": [
        "calculate_fid_given_paths(images[np.arange(30)], [root, root], 1, device, 2048)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQ-veh1gBKM_"
      },
      "source": [
        "def fitness_function(ch):\n",
        "    return calculate_fid_given_paths(images[ch], [root, root], 1, device, 2048)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djXMaJnoAVdO"
      },
      "source": [
        "POPULATION_SIZE = 100\n",
        "MUTATION_RATE = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wByJv77T-9j-",
        "collapsed": true
      },
      "source": [
        "population = []\n",
        "for i in range(POPULATION_SIZE):\n",
        "    chromosome = np.random.randint(0, 299, 30)\n",
        "    fit = fitness_function(chromosome)\n",
        "    population.append([chromosome, fit])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDw6afF5Abep"
      },
      "source": [
        "#genetic search\n",
        "\n",
        "while True:    \n",
        "    print(sorted(population, key=lambda x: x[1])[0][1])\n",
        "    fits = np.array([p[1] for p in population])\n",
        "    print(fits.mean())\n",
        "    new_population = []\n",
        "    new_population.append(sorted(population, key=lambda x: x[1])[0])\n",
        "    new_population.append(sorted(population, key=lambda x: x[1])[1])\n",
        "    while len(new_population) < POPULATION_SIZE:\n",
        "        if len(new_population) % 10 == 0:\n",
        "            print(len(new_population))\n",
        "        ch1, ch2 = np.random.choice(np.arange(POPULATION_SIZE), size=2, p=fits/fits.sum())\n",
        "\n",
        "        new_ch1 = []\n",
        "        new_ch2 = []\n",
        "\n",
        "        for i in range(30):\n",
        "            if np.random.random() < 0.5:\n",
        "                new_ch1.append(population[ch1][0][i])\n",
        "                new_ch2.append(population[ch2][0][i])\n",
        "            else:\n",
        "                new_ch1.append(population[ch2][0][i])\n",
        "                new_ch2.append(population[ch1][0][i]) \n",
        "\n",
        "        for i in range(30):\n",
        "            if np.random.random() <= MUTATION_RATE:\n",
        "                new_ch1[i] = np.random.randint(0, 299)     \n",
        "            if np.random.random() <= MUTATION_RATE:\n",
        "                new_ch2[i] = np.random.randint(0, 299)   \n",
        "\n",
        "        new_population.append([new_ch1, fitness_function(new_ch1)])\n",
        "        new_population.append([new_ch2, fitness_function(new_ch2)])\n",
        "\n",
        "    population = copy.deepcopy(new_population)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zsub1oQZJ7YN"
      },
      "source": [
        "#local optimization\n",
        "\n",
        "current = np.random.randint(0, 299, 30)\n",
        "fit = fitness_function(current)\n",
        "fit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQ21kD-FQcFL"
      },
      "source": [
        "import copy\n",
        "\n",
        "best = copy.deepcopy(current)\n",
        "best_fit = fit\n",
        "while True:\n",
        "    print(best_fit)\n",
        "    current = copy.deepcopy(best)\n",
        "    for i in range(30):\n",
        "        if np.random.random() <= (1/20):\n",
        "            current[i] = np.random.randint(0, 299)\n",
        "    cfit = fitness_function(current)\n",
        "    if cfit <= best_fit:\n",
        "        best = copy.deepcopy(current)\n",
        "        best_fit = cfit"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}